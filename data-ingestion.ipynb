{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "953618e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reportlab\n",
    "import qdrant_client\n",
    "import sentence_transformers\n",
    "import pandas\n",
    "import pypdf\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e56786a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"./realistic_drug_docs\")   # change if your folder name differs\n",
    "COLLECTION_NAME = \"drug_rag_kb\"\n",
    "\n",
    "# Choose one:\n",
    "USE_QDRANT_SERVER = True                  # recommended\n",
    "QDRANT_URL = \"http://localhost:6333\"      # if server; else ignored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1e2432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, hashlib\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1f99e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_drug_id_from_filename(name: str) -> Optional[str]:\n",
    "    m = re.search(r\"(D\\d{3})\", name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def infer_doc_type_from_filename(name: str) -> str:\n",
    "    n = name.lower()\n",
    "    if \"truth\" in n:\n",
    "        return \"truth\"\n",
    "    if \"briefingbook\" in n:\n",
    "        return \"briefingbook\"\n",
    "    return Path(name).suffix.lower().lstrip(\".\") or \"unknown\"\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.replace(\"\\x00\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def sha256_text(text: str) -> str:\n",
    "    return hashlib.sha256(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a0598160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_text(path: Path) -> str:\n",
    "    reader = PdfReader(str(path))\n",
    "    out = []\n",
    "    for page in reader.pages:\n",
    "        out.append(page.extract_text() or \"\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def load_txt_text(path: Path) -> str:\n",
    "    return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def load_json_text(path: Path) -> str:\n",
    "    obj = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "    lines = []\n",
    "    def add(k, v):\n",
    "        if v is None:\n",
    "            return\n",
    "        if isinstance(v, (str, int, float, bool)):\n",
    "            lines.append(f\"{k}: {v}\")\n",
    "        elif isinstance(v, list):\n",
    "            if not v: return\n",
    "            lines.append(f\"{k}:\")\n",
    "            for item in v:\n",
    "                if isinstance(item, dict):\n",
    "                    compact = \", \".join([f\"{ik}={item[ik]}\" for ik in item.keys()])\n",
    "                    lines.append(f\"- {compact}\")\n",
    "                else:\n",
    "                    lines.append(f\"- {item}\")\n",
    "        elif isinstance(v, dict):\n",
    "            lines.append(f\"{k}:\")\n",
    "            for dk, dv in v.items():\n",
    "                add(f\"  {dk}\", dv)\n",
    "        else:\n",
    "            lines.append(f\"{k}: {str(v)}\")\n",
    "\n",
    "    # Pull common keys if present\n",
    "    for key in [\n",
    "        \"drug_id\",\"drug_name\",\"indication\",\"modality\",\"mechanism\",\"moa_short\",\n",
    "        \"formulation\",\"route\",\"regimen\",\"trial_id\",\"phase\",\"population\",\n",
    "        \"key_inclusion\",\"key_exclusion\",\"results\",\"safety\",\"monitoring\",\n",
    "        \"version\",\"doc_date\"\n",
    "    ]:\n",
    "        if key in obj:\n",
    "            add(key, obj[key])\n",
    "\n",
    "    if not lines:\n",
    "        lines.append(json.dumps(obj, indent=2))\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def load_file(path: Path) -> Tuple[str, Dict[str, Any]]:\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == \".pdf\":\n",
    "        text = load_pdf_text(path); source_type = \"pdf\"\n",
    "    elif ext == \".txt\":\n",
    "        text = load_txt_text(path); source_type = \"txt\"\n",
    "    elif ext == \".json\":\n",
    "        text = load_json_text(path); source_type = \"json\"\n",
    "    else:\n",
    "        text = path.read_text(encoding=\"utf-8\", errors=\"ignore\"); source_type = \"unknown\"\n",
    "\n",
    "    text = normalize_text(text)\n",
    "    meta = {\n",
    "        \"file_name\": path.name,\n",
    "        \"source_type\": source_type,\n",
    "        \"drug_id\": infer_drug_id_from_filename(path.name),\n",
    "        \"doc_type\": infer_doc_type_from_filename(path.name),\n",
    "        \"abs_path\": str(path.resolve()),\n",
    "        \"doc_hash\": sha256_text(text),\n",
    "        \"text_len\": len(text),\n",
    "    }\n",
    "    return text, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bab1ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 30 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 30/30 [00:00<00:00, 106.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded docs: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assert DATA_DIR.exists(), f\"DATA_DIR not found: {DATA_DIR.resolve()}\"\n",
    "\n",
    "files = sorted([*DATA_DIR.glob(\"*.pdf\"), *DATA_DIR.glob(\"*.txt\"), *DATA_DIR.glob(\"*.json\")])\n",
    "print(\"Found:\", len(files), \"files\")\n",
    "\n",
    "docs = []\n",
    "for fp in tqdm(files, desc=\"Loading files\"):\n",
    "    text, meta = load_file(fp)\n",
    "    if len(text) < 50:\n",
    "        continue\n",
    "    docs.append({\"text\": text, \"meta\": meta})\n",
    "\n",
    "print(\"Loaded docs:\", len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "67815090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dedup: 30\n",
      "After dedup : 30\n",
      "Example sources: D005 ['pdf']\n"
     ]
    }
   ],
   "source": [
    "dedup: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "for d in docs:\n",
    "    h = d[\"meta\"][\"doc_hash\"]\n",
    "    if h not in dedup:\n",
    "        dedup[h] = {\n",
    "            \"text\": d[\"text\"],\n",
    "            \"meta\": d[\"meta\"],\n",
    "            \"source_types\": [d[\"meta\"][\"source_type\"]],\n",
    "            \"source_files\": [d[\"meta\"][\"file_name\"]],\n",
    "        }\n",
    "    else:\n",
    "        dedup[h][\"source_types\"].append(d[\"meta\"][\"source_type\"])\n",
    "        dedup[h][\"source_files\"].append(d[\"meta\"][\"file_name\"])\n",
    "\n",
    "dedup_docs = list(dedup.values())\n",
    "print(\"Before dedup:\", len(docs))\n",
    "print(\"After dedup :\", len(dedup_docs))\n",
    "print(\"Example sources:\", dedup_docs[12][\"meta\"][\"drug_id\"], dedup_docs[12][\"source_types\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5463c641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 180\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text: str, max_chars: int = 900, overlap: int = 150) -> List[str]:\n",
    "    text = normalize_text(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(n, start + max_chars)\n",
    "        c = text[start:end].strip()\n",
    "        if c:\n",
    "            chunks.append(c)\n",
    "        if end == n:\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "chunk_payloads = []\n",
    "for d in dedup_docs:\n",
    "    chunks = chunk_text(d[\"text\"])\n",
    "    meta = d[\"meta\"]\n",
    "    for i, ch in enumerate(chunks):\n",
    "        ch = normalize_text(ch)\n",
    "        if len(ch) < 50:\n",
    "            continue\n",
    "        chunk_payloads.append({\n",
    "            \"drug_id\": meta.get(\"drug_id\"),\n",
    "            \"doc_type\": meta.get(\"doc_type\"),\n",
    "            \"chunk_index\": i,\n",
    "            \"text\": ch,\n",
    "            \"chunk_hash\": sha256_text(ch),\n",
    "            \"source_types\": d[\"source_types\"],\n",
    "            \"source_files\": d[\"source_files\"],\n",
    "        })\n",
    "\n",
    "print(\"Chunks:\", len(chunk_payloads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9ba5a0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drug_id': 'D010',\n",
       " 'doc_type': 'truth',\n",
       " 'chunk_index': 2,\n",
       " 'text': 'ations: - Known hypersensitivity to active substance or excipients. monitoring: - Baseline labs per protocol (CBC, CMP) - Periodic assessment of liver enzymes - Clinical monitoring for infections version: v2.9 doc_date: 2025-12-26',\n",
       " 'chunk_hash': 'b68f13b2954e2a3ae8d7f1c9f51c38786f4364ccc9994e80a594475e5aed4c5c',\n",
       " 'source_types': ['json'],\n",
       " 'source_files': ['D010_Juvencor_TRUTH.json']}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_payloads[179]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Your openAI key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4c5498db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-3-small\"  # good + cheaper\n",
    "EMBED_DIM = 1536  # for text-embedding-3-small (dimension depends on model)\n",
    "\n",
    "def embed_texts(texts: list[str]) -> list[list[float]]:\n",
    "    # batch call\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    return [d.embedding for d in resp.data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bcd3099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from pinecone import Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cdefd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing indexes: ['test-agent']\n",
      "Using Pinecone index: test-agent\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "\n",
    "# Connect\n",
    "pc = Pinecone(api_key=\"pinecone key \")\n",
    "# Use existing index (from your screenshot)\n",
    "INDEX_NAME = \"test-agent\"\n",
    "\n",
    "existing = pc.list_indexes().names()\n",
    "print(\"Existing indexes:\", existing)\n",
    "\n",
    "if INDEX_NAME not in existing:\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=1024,          # IMPORTANT: must match index\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(INDEX_NAME)\n",
    "print(\"Using Pinecone index:\", INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d69bf1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 64\n",
    "\n",
    "def batched(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b2d4503f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3\n",
      "<class 'str'>\n",
      "drug_id: D010 drug_name: Juvencor indication: Rare-G modality: enzyme replacement mechanism: gene expression silencer mo\n"
     ]
    }
   ],
   "source": [
    "print(type(chunks))\n",
    "print(len(chunks))\n",
    "print(type(chunks[0]))\n",
    "print(str(chunks[0])[:120])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bec00b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 30\n",
      "dict_keys(['text', 'meta', 'source_types', 'source_files'])\n",
      "dict_keys(['file_name', 'source_type', 'drug_id', 'doc_type', 'abs_path', 'doc_hash', 'text_len'])\n",
      "D001\n"
     ]
    }
   ],
   "source": [
    "print(type(dedup_docs), len(dedup_docs))\n",
    "print(dedup_docs[0].keys())\n",
    "print(dedup_docs[0][\"meta\"].keys())\n",
    "print(dedup_docs[0][\"meta\"].get(\"drug_id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "19ac013e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilt chunks: 180\n",
      "Sample: D001_briefingbook_65ddfe10bfae8599 <class 'dict'> D001\n"
     ]
    }
   ],
   "source": [
    "# overwrite your current chunks (strings) with proper chunk records\n",
    "chunks = []\n",
    "\n",
    "for d in dedup_docs:\n",
    "    meta = d[\"meta\"]\n",
    "    drug_id = meta.get(\"drug_id\") or \"UNK\"\n",
    "    doc_type = meta.get(\"doc_type\") or \"doc\"\n",
    "\n",
    "    for idx, ch in enumerate(chunk_text(d[\"text\"], max_chars=900, overlap=150)):\n",
    "        ch = normalize_text(ch)\n",
    "        if len(ch) < 50:\n",
    "            continue\n",
    "\n",
    "        chunk_id = f\"{drug_id}_{doc_type}_{sha256_text(ch)[:16]}\"\n",
    "        chunks.append({\n",
    "            \"id\": chunk_id,\n",
    "            \"text\": ch,\n",
    "            \"drug_id\": drug_id,\n",
    "            \"doc_type\": doc_type,\n",
    "            \"chunk_index\": idx,\n",
    "            \"source_types\": d.get(\"source_types\", []),\n",
    "            \"source_files\": d.get(\"source_files\", []),\n",
    "        })\n",
    "\n",
    "print(\"Rebuilt chunks:\", len(chunks))\n",
    "print(\"Sample:\", chunks[0][\"id\"], type(chunks[0]), chunks[0][\"drug_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "62b6bfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embed+Upsert: 100%|██████████| 3/3 [00:27<00:00,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted vectors: 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "BATCH = 64\n",
    "NAMESPACE = \"toy-agent\"\n",
    "\n",
    "def batched(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "total = 0\n",
    "\n",
    "for batch in tqdm(list(batched(chunks, BATCH)), desc=\"Embed+Upsert\"):\n",
    "    embedded = pc.inference.embed(\n",
    "        model=\"llama-text-embed-v2\",\n",
    "        inputs=[b[\"text\"] for b in batch],\n",
    "        parameters={\"input_type\": \"passage\"}\n",
    "    )\n",
    "\n",
    "    # ✅ correct for EmbeddingsList object\n",
    "    vectors = [e.values for e in embedded.data]\n",
    "\n",
    "    to_upsert = []\n",
    "    for b, vec in zip(batch, vectors):\n",
    "        meta = {\n",
    "            \"drug_id\": b[\"drug_id\"],\n",
    "            \"doc_type\": b[\"doc_type\"],\n",
    "            \"chunk_index\": b[\"chunk_index\"],\n",
    "            \"text\": b[\"text\"],\n",
    "            \"source_types\": b[\"source_types\"],\n",
    "            \"source_files\": b[\"source_files\"],\n",
    "        }\n",
    "        to_upsert.append((b[\"id\"], vec, meta))\n",
    "\n",
    "    index.upsert(vectors=to_upsert, namespace=NAMESPACE)\n",
    "    total += len(to_upsert)\n",
    "\n",
    "print(\"Upserted vectors:\", total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf539b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
